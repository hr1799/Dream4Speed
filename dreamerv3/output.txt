root@VibhakarPC:/home/dreamerv3# python3 example.py 
Encoder CNN shapes: {'image': (64, 64, 3)}
Encoder MLP shapes: {}
Decoder CNN shapes: {'image': (64, 64, 3)}
Decoder MLP shapes: {}
JAX devices (1): [cuda(id=0)]
Policy devices: cuda:0
Train devices:  cuda:0
2024-04-09 00:59:40.235999: W external/xla/xla/service/gpu/nvptx_compiler.cc:718] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.4.131). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
Tracing train function.
Optimizer model_opt has 32,468,675 variables.
Optimizer actor_opt has 2,144,657 variables.
Optimizer critic_opt has 2,297,215 variables.
Logdir /root/logdir/run1
Observation space:
  image            Space(dtype=uint8, shape=(64, 64, 3), low=0, high=255)
  reward           Space(dtype=float32, shape=(), low=-inf, high=inf)
  is_first         Space(dtype=bool, shape=(), low=False, high=True)
  is_last          Space(dtype=bool, shape=(), low=False, high=True)
  is_terminal      Space(dtype=bool, shape=(), low=False, high=True)
Action space:
  action           Space(dtype=float32, shape=(17,), low=0, high=1)
  reset            Space(dtype=bool, shape=(), low=False, high=True)
Prefill train dataset.
Episode has 204 steps and return 2.1.
Episode has 162 steps and return 1.1.
Episode has 136 steps and return 0.1.
Episode has 213 steps and return 3.1.
Episode has 216 steps and return 1.1.
Episode has 163 steps and return 1.1.
─────────────────────────────────────────────────────────────────────────── Step 1100 ───────────────────────────────────────────────────────────────────────────
episode/length 163 / episode/score 1.1 / episode/sum_abs_reward 2.7 / episode/reward_rate 0.01

Creating new TensorBoard event file writer.
Did not find any checkpoint.
Writing checkpoint: /root/logdir/run1/checkpoint.ckpt
Start training loop.
Saved chunk: 20240409T005945F309521-5b6ByhjXTAxvZ60LMBmUNe-5pyGVu8OrLeUauNKDzoMVt-1024.npz
Saved chunk: 20240409T005950F740444-5pyGVu8OrLeUauNKDzoMVt-0000000000000000000000-76.npz
Wrote checkpoint: /root/logdir/run1/checkpoint.ckpt
Tracing policy function.
Tracing policy function.
Tracing train function.
2024-04-09 01:00:13.092884: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f16[1024,32,32,48]{3,2,1,0}, u8[0]{0}) custom-call(f16[1024,35,35,96]{3,2,1,0}, f16[48,4,4,96]{3,2,1,0}), window={size=4x4}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convForward", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
2024-04-09 01:00:15.021829: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.929049957s
Trying algorithm eng0{} for conv (f16[1024,32,32,48]{3,2,1,0}, u8[0]{0}) custom-call(f16[1024,35,35,96]{3,2,1,0}, f16[48,4,4,96]{3,2,1,0}), window={size=4x4}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convForward", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
2024-04-09 01:00:25.988681: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f16[1024,32,32,48]{3,2,1,0}, u8[0]{0}) custom-call(f16[1024,16,16,96]{3,2,1,0}, f16[96,4,4,48]{3,2,1,0}), window={size=4x4 stride=2x2 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convBackwardInput", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
2024-04-09 01:00:26.555277: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.566753486s
Trying algorithm eng0{} for conv (f16[1024,32,32,48]{3,2,1,0}, u8[0]{0}) custom-call(f16[1024,16,16,96]{3,2,1,0}, f16[96,4,4,48]{3,2,1,0}), window={size=4x4 stride=2x2 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convBackwardInput", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
2024-04-09 01:00:28.003701: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f16[48,4,4,3]{3,2,1,0}, u8[0]{0}) custom-call(f16[1024,64,64,3]{3,2,1,0}, f16[1024,32,32,48]{3,2,1,0}), window={size=4x4 stride=2x2 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convBackwardFilter", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
2024-04-09 01:00:30.002858: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.999240843s
Trying algorithm eng0{} for conv (f16[48,4,4,3]{3,2,1,0}, u8[0]{0}) custom-call(f16[1024,64,64,3]{3,2,1,0}, f16[1024,32,32,48]{3,2,1,0}), window={size=4x4 stride=2x2 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convBackwardFilter", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
Tracing report function.
Tracing report function.
2024-04-09 01:00:55.073015: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f16[354,32,32,48]{3,2,1,0}, u8[0]{0}) custom-call(f16[354,35,35,96]{3,2,1,0}, f16[48,4,4,96]{3,2,1,0}), window={size=4x4}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convForward", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
2024-04-09 01:00:55.087299: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.014432384s
Trying algorithm eng0{} for conv (f16[354,32,32,48]{3,2,1,0}, u8[0]{0}) custom-call(f16[354,35,35,96]{3,2,1,0}, f16[48,4,4,96]{3,2,1,0}), window={size=4x4}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convForward", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
─────────────────────────────────────────────────────────────────────────── Step 1101 ───────────────────────────────────────────────────────────────────────────
train/action_mag 16 / train/action_max 16 / train/action_mean 8.01 / train/action_min 0 / train/action_std 5.06 / train/actor_opt_actor_opt_grad_overflow 0 / 
train/actor_opt_actor_opt_grad_scale 1e4 / train/actor_opt_grad_norm 1.8e-4 / train/actor_opt_grad_steps 1 / train/actor_opt_loss -2.47 / train/adv_mag 0 / 
train/adv_max 0 / train/adv_mean 0 / train/adv_min 0 / train/adv_std 0 / train/cont_avg 1 / train/cont_loss_mean 0.6 / train/cont_loss_std 0.29 / 
train/cont_neg_acc 0 / train/cont_neg_loss 1.07 / train/cont_pos_acc 0.68 / train/cont_pos_loss 0.6 / train/cont_pred 0.57 / train/cont_rate 1 / 
train/dyn_loss_mean 7.9 / train/dyn_loss_std 0.32 / train/extr_critic_critic_opt_critic_opt_grad_overflow 0 / train/extr_critic_critic_opt_critic_opt_grad_scale 
1e4 / train/extr_critic_critic_opt_grad_norm 6.15 / train/extr_critic_critic_opt_grad_steps 1 / train/extr_critic_critic_opt_loss 3.5e4 / train/extr_critic_mag 0
/ train/extr_critic_max 0 / train/extr_critic_mean 0 / train/extr_critic_min 0 / train/extr_critic_std 0 / train/extr_return_normed_mag 0 / 
train/extr_return_normed_max 0 / train/extr_return_normed_mean 0 / train/extr_return_normed_min 0 / train/extr_return_normed_std 0 / train/extr_return_rate 0 / 
train/extr_return_raw_mag 0 / train/extr_return_raw_max 0 / train/extr_return_raw_mean 0 / train/extr_return_raw_min 0 / train/extr_return_raw_std 0 / 
train/extr_reward_mag 0 / train/extr_reward_max 0 / train/extr_reward_mean 0 / train/extr_reward_min 0 / train/extr_reward_std 0 / train/image_loss_mean 3477.38 
/ train/image_loss_std 171.53 / train/model_loss_mean 3488.26 / train/model_loss_std 171.57 / train/model_opt_grad_norm nan / train/model_opt_grad_steps 0 / 
train/model_opt_loss 3.5e7 / train/model_opt_model_opt_grad_overflow 1 / train/model_opt_model_opt_grad_scale 5000 / train/policy_entropy_mag 2.79 / 
train/policy_entropy_max 2.79 / train/policy_entropy_mean 2.6 / train/policy_entropy_min 1.88 / train/policy_entropy_std 0.09 / train/policy_logprob_mag 5.37 / 
train/policy_logprob_max -0.6 / train/policy_logprob_mean -2.6 / train/policy_logprob_min -5.37 / train/policy_logprob_std 0.67 / train/policy_randomness_mag 
0.99 / train/policy_randomness_max 0.99 / train/policy_randomness_mean 0.92 / train/policy_randomness_min 0.66 / train/policy_randomness_std 0.03 / 
train/post_ent_mag 107.36 / train/post_ent_max 107.36 / train/post_ent_mean 106.99 / train/post_ent_min 106.62 / train/post_ent_std 0.13 / train/prior_ent_mag 
107.67 / train/prior_ent_max 107.67 / train/prior_ent_mean 106.88 / train/prior_ent_min 106.09 / train/prior_ent_std 0.24 / train/rep_loss_mean 7.9 / 
train/rep_loss_std 0.32 / train/reward_avg 0.01 / train/reward_loss_mean 5.54 / train/reward_loss_std 9.5e-7 / train/reward_max_data 1 / train/reward_max_pred 0 
/ train/reward_neg_acc 1 / train/reward_neg_loss 5.54 / train/reward_pos_acc 0 / train/reward_pos_loss 5.54 / train/reward_pred 0 / train/reward_rate 0.02 / 
train/params_agent/wm/model_opt 3.2e7 / train/params_agent/task_behavior/critic/critic_opt 2.3e6 / train/params_agent/task_behavior/ac/actor_opt 2.1e6 / 
replay/size 1038 / replay/inserts 1038 / replay/samples 112 / replay/insert_wait_avg 9.8e-7 / replay/insert_wait_frac 1 / replay/sample_wait_avg 1.2e-6 / 
replay/sample_wait_frac 1 / timer/duration 74.01 / timer/env.step_count 1101 / timer/env.step_total 7.59 / timer/env.step_frac 0.1 / timer/env.step_avg 6.9e-3 / 
timer/env.step_min 1e-3 / timer/env.step_max 1.37 / timer/replay.add_count 1101 / timer/replay.add_total 0.04 / timer/replay.add_frac 5.8e-4 / 
timer/replay.add_avg 3.9e-5 / timer/replay.add_min 1.5e-5 / timer/replay.add_max 8.5e-3 / timer/logger.write_count 1 / timer/logger.write_total 0.02 / 
timer/logger.write_frac 2.7e-4 / timer/logger.write_avg 0.02 / timer/logger.write_min 0.02 / timer/logger.write_max 0.02 / timer/checkpoint.save_count 1 / 
timer/checkpoint.save_total 0.05 / timer/checkpoint.save_frac 6.7e-4 / timer/checkpoint.save_avg 0.05 / timer/checkpoint.save_min 0.05 / 
timer/checkpoint.save_max 0.05 / timer/agent.save_count 1 / timer/agent.save_total 0.45 / timer/agent.save_frac 6.1e-3 / timer/agent.save_avg 0.45 / 
timer/agent.save_min 0.45 / timer/agent.save_max 0.45 / timer/replay.save_count 1 / timer/replay.save_total 1.3e-4 / timer/replay.save_frac 1.8e-6 / 
timer/replay.save_avg 1.3e-4 / timer/replay.save_min 1.3e-4 / timer/replay.save_max 1.3e-4 / timer/agent.policy_count 1 / timer/agent.policy_total 2.99 / 
timer/agent.policy_frac 0.04 / timer/agent.policy_avg 2.99 / timer/agent.policy_min 2.99 / timer/agent.policy_max 2.99 / timer/dataset_count 1 / 
timer/dataset_total 1.7e-5 / timer/dataset_frac 2.3e-7 / timer/dataset_avg 1.7e-5 / timer/dataset_min 1.7e-5 / timer/dataset_max 1.7e-5 / timer/agent.train_count
1 / timer/agent.train_total 53.99 / timer/agent.train_frac 0.73 / timer/agent.train_avg 53.99 / timer/agent.train_min 53.99 / timer/agent.train_max 53.99 / 
timer/agent.report_count 1 / timer/agent.report_total 9.25 / timer/agent.report_frac 0.12 / timer/agent.report_avg 9.25 / timer/agent.report_min 9.25 / 
timer/agent.report_max 9.25

Episode has 148 steps and return 0.1.
Episode has 169 steps and return 0.1.
Episode has 163 steps and return 0.1.
Episode has 104 steps and return 0.1.
─────────────────────────────────────────────────────────────────────────── Step 1821 ───────────────────────────────────────────────────────────────────────────
episode/length 104 / episode/score 0.1 / episode/sum_abs_reward 2.7 / episode/reward_rate 9.5e-3 / train/action_mag 16 / train/action_max 16 / train/action_mean 
7.97 / train/action_min 0 / train/action_std 4.9 / train/actor_opt_actor_opt_grad_overflow 0 / train/actor_opt_actor_opt_grad_scale 1e4 / 
train/actor_opt_grad_norm 4.2e-3 / train/actor_opt_grad_steps 25 / train/actor_opt_loss 248.83 / train/adv_mag 0.02 / train/adv_max 0.02 / train/adv_mean 9.6e-3 
/ train/adv_min 1.5e-3 / train/adv_std 4.4e-3 / train/cont_avg 0.99 / train/cont_loss_mean 0.18 / train/cont_loss_std 0.43 / train/cont_neg_acc 0.03 / 
train/cont_neg_loss 4.96 / train/cont_pos_acc 0.93 / train/cont_pos_loss 0.15 / train/cont_pred 0.89 / train/cont_rate 0.99 / train/dyn_loss_mean 5.98 / 
train/dyn_loss_std 0.72 / train/extr_critic_critic_opt_critic_opt_grad_overflow 0 / train/extr_critic_critic_opt_critic_opt_grad_scale 1e4 / 
train/extr_critic_critic_opt_grad_norm 25.39 / train/extr_critic_critic_opt_grad_steps 25 / train/extr_critic_critic_opt_loss 1e5 / train/extr_critic_mag 1.7e-4 
/ train/extr_critic_max 1.7e-4 / train/extr_critic_mean 1.7e-4 / train/extr_critic_min 1.3e-4 / train/extr_critic_std 1.9e-6 / train/extr_return_normed_mag 0.02 
/ train/extr_return_normed_max 0.02 / train/extr_return_normed_mean 9.5e-3 / train/extr_return_normed_min 1.4e-3 / train/extr_return_normed_std 4.4e-3 / 
train/extr_return_rate 0 / train/extr_return_raw_mag 0.02 / train/extr_return_raw_max 0.02 / train/extr_return_raw_mean 9.7e-3 / train/extr_return_raw_min 1.7e-3
/ train/extr_return_raw_std 4.4e-3 / train/extr_reward_mag 1.5e-3 / train/extr_reward_max 1.5e-3 / train/extr_reward_mean 1.5e-3 / train/extr_reward_min 1.4e-3 /
train/extr_reward_std 7.4e-6 / train/image_loss_mean 1262.4 / train/image_loss_std 125.66 / train/model_loss_mean 1270.8 / train/model_loss_std 125.74 / 
train/model_opt_grad_norm 3603.13 / train/model_opt_grad_steps 16 / train/model_opt_loss 2.5e4 / train/model_opt_model_opt_grad_overflow 0 / 
train/model_opt_model_opt_grad_scale 19.53 / train/policy_entropy_mag 2.8 / train/policy_entropy_max 2.8 / train/policy_entropy_mean 2.71 / 
train/policy_entropy_min 2.29 / train/policy_entropy_std 0.04 / train/policy_logprob_mag 4.67 / train/policy_logprob_max -1.19 / train/policy_logprob_mean -2.71 
/ train/policy_logprob_min -4.67 / train/policy_logprob_std 0.47 / train/policy_randomness_mag 0.99 / train/policy_randomness_max 0.99 / 
train/policy_randomness_mean 0.96 / train/policy_randomness_min 0.81 / train/policy_randomness_std 0.01 / train/post_ent_mag 98.29 / train/post_ent_max 98.29 / 
train/post_ent_mean 96.92 / train/post_ent_min 96.23 / train/post_ent_std 0.5 / train/prior_ent_mag 104.46 / train/prior_ent_max 104.46 / train/prior_ent_mean 
102.5 / train/prior_ent_min 101.97 / train/prior_ent_std 0.41 / train/rep_loss_mean 5.98 / train/rep_loss_std 0.72 / train/reward_avg 2.3e-3 / 
train/reward_loss_mean 4.64 / train/reward_loss_std 0.06 / train/reward_max_data 1 / train/reward_max_pred 1.5e-3 / train/reward_neg_acc 1 / 
train/reward_neg_loss 4.64 / train/reward_pos_acc 0 / train/reward_pos_loss 5.13 / train/reward_pred 1.5e-3 / train/reward_rate 9.3e-3 / stats/mean_log_entropy 
2.66 / replay/size 1758 / replay/inserts 720 / replay/samples 720 / replay/insert_wait_avg 1.8e-6 / replay/insert_wait_frac 1 / replay/sample_wait_avg 1.2e-6 / 
replay/sample_wait_frac 1 / timer/duration 21.02 / timer/env.step_count 720 / timer/env.step_total 4.83 / timer/env.step_frac 0.23 / timer/env.step_avg 6.7e-3 / 
timer/env.step_min 1.2e-3 / timer/env.step_max 0.94 / timer/replay.add_count 720 / timer/replay.add_total 0.05 / timer/replay.add_frac 2.5e-3 / 
timer/replay.add_avg 7.2e-5 / timer/replay.add_min 3.6e-5 / timer/replay.add_max 3.1e-4 / timer/logger.write_count 1 / timer/logger.write_total 0.01 / 
timer/logger.write_frac 6.7e-4 / timer/logger.write_avg 0.01 / timer/logger.write_min 0.01 / timer/logger.write_max 0.01 / timer/checkpoint.save_count 0 / 
timer/checkpoint.save_total 0 / timer/checkpoint.save_frac 0 / timer/agent.save_count 0 / timer/agent.save_total 0 / timer/agent.save_frac 0 / 
timer/replay.save_count 0 / timer/replay.save_total 0 / timer/replay.save_frac 0 / timer/agent.policy_count 720 / timer/agent.policy_total 1.41 / 
timer/agent.policy_frac 0.07 / timer/agent.policy_avg 2e-3 / timer/agent.policy_min 1.5e-3 / timer/agent.policy_max 5.2e-3 / timer/dataset_count 45 / 
timer/dataset_total 2.5e-3 / timer/dataset_frac 1.2e-4 / timer/dataset_avg 5.5e-5 / timer/dataset_min 4.4e-5 / timer/dataset_max 1.3e-4 / timer/agent.train_count
45 / timer/agent.train_total 14.51 / timer/agent.train_frac 0.69 / timer/agent.train_avg 0.32 / timer/agent.train_min 0.3 / timer/agent.train_max 0.44 / 
timer/agent.report_count 1 / timer/agent.report_total 0.14 / timer/agent.report_frac 6.7e-3 / timer/agent.report_avg 0.14 / timer/agent.report_min 0.14 / 
timer/agent.report_max 0.14 / fps 34.25

Episode has 256 steps and return 4.1.
Saved chunk: 20240409T005950F740444-5pyGVu8OrLeUauNKDzoMVt-4dg2g3SX6jswWkLdwMndWq-1024.npz
Episode has 154 steps and return 0.1.
Episode has 207 steps and return 2.1.
Episode has 253 steps and return 3.1.
Episode has 165 steps and return 3.1.
─────────────────────────────────────────────────────────────────────────── Step 2909 ───────────────────────────────────────────────────────────────────────────
episode/length 165 / episode/score 3.1 / episode/sum_abs_reward 4.7 / episode/reward_rate 0.02 / train/action_mag 16 / train/action_max 16 / train/action_mean 
8.07 / train/action_min 0 / train/action_std 5.05 / train/actor_opt_actor_opt_grad_overflow 0 / train/actor_opt_actor_opt_grad_scale 1e4 / 
train/actor_opt_grad_norm 3.5e-3 / train/actor_opt_grad_steps 80 / train/actor_opt_loss 236.95 / train/adv_mag 0.02 / train/adv_max 0.02 / train/adv_mean 9e-3 / 
train/adv_min 1.2e-3 / train/adv_std 4.3e-3 / train/cont_avg 0.99 / train/cont_loss_mean 0.03 / train/cont_loss_std 0.34 / train/cont_neg_acc 0 / 
train/cont_neg_loss 4.9 / train/cont_pos_acc 1 / train/cont_pos_loss 6.2e-3 / train/cont_pred 0.99 / train/cont_rate 0.99 / train/dyn_loss_mean 7.84 / 
train/dyn_loss_std 5.76 / train/extr_critic_critic_opt_critic_opt_grad_overflow 0 / train/extr_critic_critic_opt_critic_opt_grad_scale 1e4 / 
train/extr_critic_critic_opt_grad_norm 28.94 / train/extr_critic_critic_opt_grad_steps 80 / train/extr_critic_critic_opt_loss 7.6e4 / train/extr_critic_mag 
1.6e-3 / train/extr_critic_max 1.6e-3 / train/extr_critic_mean 1.6e-3 / train/extr_critic_min 1.5e-3 / train/extr_critic_std 5.1e-6 / 
train/extr_return_normed_mag 0.02 / train/extr_return_normed_max 0.02 / train/extr_return_normed_mean 9.7e-3 / train/extr_return_normed_min 1.9e-3 / 
train/extr_return_normed_std 4.3e-3 / train/extr_return_rate 0 / train/extr_return_raw_mag 0.02 / train/extr_return_raw_max 0.02 / train/extr_return_raw_mean 
0.01 / train/extr_return_raw_min 2.8e-3 / train/extr_return_raw_std 4.3e-3 / train/extr_reward_mag 1.5e-3 / train/extr_reward_max 1.4e-3 / train/extr_reward_mean
1.4e-3 / train/extr_reward_min 1.4e-3 / train/extr_reward_std 3.3e-6 / train/image_loss_mean 198.8 / train/image_loss_std 116.59 / train/model_loss_mean 204.98 /
train/model_loss_std 116.64 / train/model_opt_grad_norm 447.52 / train/model_opt_grad_steps 71 / train/model_opt_loss 4003.45 / 
train/model_opt_model_opt_grad_overflow 0 / train/model_opt_model_opt_grad_scale 19.53 / train/policy_entropy_mag 2.82 / train/policy_entropy_max 2.82 / 
train/policy_entropy_mean 2.78 / train/policy_entropy_min 2.56 / train/policy_entropy_std 0.02 / train/policy_logprob_mag 4.28 / train/policy_logprob_max -1.52 /
train/policy_logprob_mean -2.78 / train/policy_logprob_min -4.28 / train/policy_logprob_std 0.33 / train/policy_randomness_mag 1 / train/policy_randomness_max 1 
/ train/policy_randomness_mean 0.98 / train/policy_randomness_min 0.9 / train/policy_randomness_std 7e-3 / train/post_ent_mag 85.65 / train/post_ent_max 85.65 / 
train/post_ent_mean 69.67 / train/post_ent_min 63.45 / train/post_ent_std 5.58 / train/prior_ent_mag 85.69 / train/prior_ent_max 85.69 / train/prior_ent_mean 
81.05 / train/prior_ent_min 78.85 / train/prior_ent_std 1.84 / train/rep_loss_mean 7.84 / train/rep_loss_std 5.76 / train/reward_avg 8e-3 / 
train/reward_loss_mean 1.43 / train/reward_loss_std 0.6 / train/reward_max_data 1 / train/reward_max_pred 1.4e-3 / train/reward_neg_acc 1 / train/reward_neg_loss
1.39 / train/reward_pos_acc 0 / train/reward_pos_loss 5.01 / train/reward_pred 1.4e-3 / train/reward_rate 0.01 / stats/mean_log_entropy 2.77 / replay/size 2846 /
replay/inserts 1088 / replay/samples 1088 / replay/insert_wait_avg 1.5e-6 / replay/insert_wait_frac 1 / replay/sample_wait_avg 9.7e-7 / replay/sample_wait_frac 1
/ timer/duration 30.3 / timer/env.step_count 1088 / timer/env.step_total 6.1 / timer/env.step_frac 0.2 / timer/env.step_avg 5.6e-3 / timer/env.step_min 1.1e-3 / 
timer/env.step_max 0.87 / timer/replay.add_count 1088 / timer/replay.add_total 0.06 / timer/replay.add_frac 2e-3 / timer/replay.add_avg 5.5e-5 / 
timer/replay.add_min 2.9e-5 / timer/replay.add_max 5.2e-4 / timer/logger.write_count 1 / timer/logger.write_total 0.01 / timer/logger.write_frac 4.6e-4 / 
timer/logger.write_avg 0.01 / timer/logger.write_min 0.01 / timer/logger.write_max 0.01 / timer/checkpoint.save_count 0 / timer/checkpoint.save_total 0 / 
timer/checkpoint.save_frac 0 / timer/agent.save_count 0 / timer/agent.save_total 0 / timer/agent.save_frac 0 / timer/replay.save_count 0 / 
timer/replay.save_total 0 / timer/replay.save_frac 0 / timer/agent.policy_count 1088 / timer/agent.policy_total 2.06 / timer/agent.policy_frac 0.07 / 
timer/agent.policy_avg 1.9e-3 / timer/agent.policy_min 1.5e-3 / timer/agent.policy_max 6.4e-3 / timer/dataset_count 68 / timer/dataset_total 3.7e-3 / 
timer/dataset_frac 1.2e-4 / timer/dataset_avg 5.5e-5 / timer/dataset_min 4.2e-5 / timer/dataset_max 1.1e-4 / timer/agent.train_count 68 / timer/agent.train_total
21.85 / timer/agent.train_frac 0.72 / timer/agent.train_avg 0.32 / timer/agent.train_min 0.3 / timer/agent.train_max 0.43 / timer/agent.report_count 1 / 
timer/agent.report_total 0.14 / timer/agent.report_frac 4.6e-3 / timer/agent.report_avg 0.14 / timer/agent.report_min 0.14 / timer/agent.report_max 0.14 / fps 
35.91

Episode has 205 steps and return 0.1.